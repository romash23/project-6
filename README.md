# <center> PROJECT-6. Задача кластеризации. Реальный кейс от компании IntelliVision

В данном проекте попробуем решить реальный кейс от компании IntelliVision. Нам предстоит исследовать возможность применения алгоритмов кластеризации для разметки новых данных и поиска выбросов.

### Техническая задача специалиста Data Science

Построить модель кластеризации изображений на основе дескрипторов, выделяемых с помощью различных архитектур нейронных сетей, проинтерпретировать полученные результаты и выбрать модель или комбинацию моделей, которая выделяет наиболее пригодные для интерпретации признаки.

### Информация о данных

Исходная папка с данными имеет следующую структуру:

<img src=Images/Screen1.jpg>

Давайте разберёмся в ней:

>В папке descriptors содержатся дескрипторы, полученные для каждого из изображений с помощью соответствующих нейронных сетей, в формате numpy-массивов, сохранённых в файлах .pickle:

   - *efficientnet-b7.pickle* — дескрипторы, выделенные моделью классификации с архитектурой **EfficientNet** версии 7. Эта модель является свёрточной нейронной сетью, предобученной на датасете **ImageNet**, в котором содержатся изображения более 1000 различных классов. Эта модель при обучении не видела датасета **veriwiId**.
   - *osnet.pickle* — дескрипторы, выделенные моделью OSNet, обученной для детектирования людей, животных и машин. Модель не обучалась на исходном датасете **veriwiId**.
   - *vdc_color.pickle* — дескрипторы, выделенные моделью регрессии для определения цвета транспортных средств в формате RGB. Частично обучена на исходном датасете **veriwild**.
   - *vdc_type.pickle* — дескрипторы, выделенные моделью классификации транспортных средств по типу на десяти классах. Частично обучена на исходном датасете **veriwild**.

>В папке raw_data содержится zip-архив с исходными изображениями автомобилей. Распакуйте его содержимое в папку raw_data. Архив содержит десять папок с изображениями, пронумерованных от 1 до 10. Каждая папка содержит подпапки, обозначенные пятизначными цифрами, например 36191.

>В каждой из таких подпапок содержатся фотографии одного конкретного автомобиля с разных ракурсов, снятые с помощью дорожных видеокамер.

<img src=Images/Screen2.jpg>

    В файле images_paths.csv представлен список из полных путей до изображений.
    Он нужен для анализа изображений, попавших в определённый кластер.

С самими данными можно ознакомиться по [ссылке](https://www.kaggle.com/datasets/markhomeless/intellivision-case)

### Работа над проектом

Как в общем-то любой задачи специалиста DS необходимо было предварительно изуть данные, с которыми предстояло работать. После импорта необходимых библиотек я прочитал файлы, храняшие в себе исходные дескрипторы и посмотрели на их размерности

```py
Размерность efficientnet-b7.pickle:  (416314, 2560)
Размерность osnet.pickle:  (416314, 512)
Размерность vdc_color.pickle:  (416314, 128)
Размерность vdc_type.pickle:  (416314, 512)
```

Оказалось, что признаки, найденные с помощью некоторых моделей, исчисляются тысячами, что довольно много, учитывая общее количество наблюдений. Очевидно, производить кластеризацию на таком большом количестве признаков, которые были сформированы исходными моделями глубокого обучения, довольно сложно и затратно по времени. К тому же, многие признаки, найденные моделями на изображениях, могут быть сильно скоррелированы между собой.

Для того, чтобы не работать с такими неудобными данными в лоб мы произвели понижение размерности исходных дескрипторов с помощью *метода главных компонент (PCA)*.

Для кластеризации данных желательно было бы узнать оптимальное число кластеров. Для это мы обратились к внутренними мерам (метрикам): индексу Калински — Харабаса и индексу Дэвиса — Болдина.

Следующим шагов была непосредственно кластеризация и визуализация полученных результатов. Даже после понижения размерности данных визуализировать кластеры было бы невозможно, поэтому мне пришлось повторно понизить размерность данных до 2х измерений, чтобы можно было бы увидеть сами кластера на экране компьютера.

Желательно для этого было бы использовать нелинейное понижение размерности, чтобы сохранить важные структуральные и топологические свойства данных, например, метод *t-SNE*. Но тут я столкнулся с первой трудностью: при работе в классической *sklearn* и такими большими данными понижение размерности одного дескриптора заняло у меня 354 минуты реального времени (~ 6 часов). Получается, чтобы понизить размерность всех данных пришлось бы ждать около суток. Для ускорения процесса я попытался воспользоваться библиотекой *CuML* в среде *Google Colab*. Но к моему огромному сожалению после установки в среду всех нужных библиотек в бесплатной версии *Colab* оставалось слишком мало свободного места: при загрузки 30Гб данных с *Kaggle* сервис тут же удалял их и предлагал повысить версию до *Colab Pro* или выше :pensive:

Поскольку проект учебный я решил, воспользоваться линейными методами понижения размености, осознавая, что часть данных будет утеряна и вероятно кластеризация получится не самая удачная.

При выборе методов кластеризациия я снова столкнулся с трудностью: выбранный мною алгоритм иерархическую кластеризации *AgglomerativeClustering* не хотел запускаться на моем ноутбуке и вызывали такую ошибку:

<img src=Images/Screen3.jpg>

А вот алгоритм кластеризации на основе плотности DBSCAN хоть и запустился с параметрами по умолчанию, но насчитал в данных более 1000 кластеров, что конечно же меня не устроило. Я попробовал увеличить радиус eps, чтобы уменьшить количество кластеров, но это привело только к нехватке оперативной памяти (злосчатная *MemoryError*). Поэтому мною было принято решение использовать менее требовательные к вычислительным мощностям алгоритмы.

Последним шагом было небольшое пребразование данной в модуле функции *plot_samples_images()* для визуализации случайных фотографии автомобилей кластеров и дальнейшей их интерпретации.

### Результат работы

Результат работы представлен в [jupiter-ноутбуке](https://github.com/romash23/project-6/blob/master/PROJECT-6._%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9_%D1%82%D1%80%D0%B0%D0%BD%D1%81%D0%BF%D0%BE%D1%80%D1%82%D0%BD%D1%8B%D1%85_%D1%81%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B2.ipynb)

### Выводы










В данном проекте мы на практике мы попробовали свои силы на реальном кейсе компании Booking - построена модель машинного обучения для предсказания рейтинга отелей. Проведённый разведовательный анализ данных (*EDA*) позволил выявить и устранить дубликаты, а также преобразовать категориальные признаки в числовые, что улучшило качество данных для модели. Особое внимание уделялось борьбе с мультиколлинеарностью и нормализации признаков, что позволило повысить стабильность и точность модели.

Использование алгоритма *RandomForestRegressor* показало хорошую эффективность в задаче регрессии рейтингов, что подтверждается достигнутой метрикой качества на *Kaggle*. Проект продемонстрировал важность тщательной подготовки данных и грамотного инжиниринга признаков (*Feature Engineering*) для повышения точности предсказаний.

Таким образом, данный проект стал успешным примером полного цикла работы с данными - от их изучения и обработки до построения и оценки модели, что является ключевым навыком для дата-сайентиста и позволяет выявлять аномалии в бизнес-процессах, такие как нечестные отели.

